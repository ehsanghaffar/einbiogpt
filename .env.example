# ============================================
# LLM Model Selection
# ============================================
# Choose one of: gpt-4o (default), gpt-5, glm-4.7-flash-free, gemini-2.0-flash-free,mimo-v2-flash-free or any AIHUBMIX model
# See docs/LLM_MODEL_CONFIGURATION.md for details
NEXT_LLM_MODEL=mimo-v2-flash-free

# ============================================
# OpenAI Configuration
# ============================================
# Required for: gpt-4o, gpt-5
# Get from: https://platform.openai.com/api-keys
# NOTE: Server-side only, NOT prefixed with NEXT_PUBLIC_
NEXT_OPENAI_API_KEY=sk_live_your_key_here

# ============================================
# AIHUBMIX Configuration  
# ============================================
# Required for: glm-4.7-flash-free, gemini-2.0-flash-free, and other free models
# Get from: https://aihubmix.com
AIHUBMIX_API_KEY=your_aihubmix_key_here
AIHUBMIX_API_BASE_URL=https://aihubmix.com/v1

# ============================================
# Rate Limiting
# ============================================
# Upstash Redis for production rate limiting (optional - falls back to in-memory if not set)
REDIS_URL=
REDIS_TOKEN=

# Rate limiting window (milliseconds)
RATE_LIMIT_WINDOW=60000

# Maximum requests per window per IP
RATE_LIMIT_MAX=10

# ============================================
# Client-side Settings
# ============================================
# Cooldown between requests (seconds)
NEXT_PUBLIC_COOLDOWN_TIME=10

# ============================================
# Quick Start
# ============================================
# 1. Using GPT-4o (requires OpenAI key):
#    NEXT_LLM_MODEL=gpt-4o
#    NEXT_OPENAI_API_KEY=sk_...
#
# 2. Using free models (requires AIHUBMIX key):
#    NEXT_LLM_MODEL=glm-4.7-flash-free
#    AIHUBMIX_API_KEY=...
#
# For detailed setup, see docs/ENV_VARIABLES.md